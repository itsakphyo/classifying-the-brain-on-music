{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "ae96753c",
   "metadata": {},
   "source": [
    "# Model Selection and Comparison\n",
    "\n",
    "This notebook compares different machine learning algorithms for brain activity classification.\n",
    "\n",
    "## Objectives:\n",
    "- Prepare data for modeling\n",
    "- Implement multiple ML algorithms\n",
    "- Compare model performance\n",
    "- Select the best performing model\n",
    "- Tune hyperparameters\n",
    "- Evaluate final model"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "15fcd789",
   "metadata": {},
   "source": [
    "## 1. Import Required Libraries"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "be5040ba",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Data manipulation\n",
    "import pandas as pd\n",
    "import numpy as np\n",
    "import json\n",
    "\n",
    "# Visualization\n",
    "import matplotlib.pyplot as plt\n",
    "import seaborn as sns\n",
    "import plotly.express as px\n",
    "\n",
    "# Preprocessing\n",
    "from sklearn.model_selection import train_test_split, cross_val_score, StratifiedKFold\n",
    "from sklearn.preprocessing import StandardScaler, LabelEncoder\n",
    "from sklearn.feature_selection import SelectKBest, f_classif, RFE\n",
    "\n",
    "# Models\n",
    "from sklearn.linear_model import LogisticRegression\n",
    "from sklearn.ensemble import RandomForestClassifier, GradientBoostingClassifier\n",
    "from sklearn.svm import SVC\n",
    "from sklearn.neighbors import KNeighborsClassifier\n",
    "from sklearn.naive_bayes import GaussianNB\n",
    "from sklearn.tree import DecisionTreeClassifier\n",
    "import xgboost as xgb\n",
    "import lightgbm as lgb\n",
    "\n",
    "# Metrics\n",
    "from sklearn.metrics import (\n",
    "    accuracy_score, precision_score, recall_score, f1_score,\n",
    "    classification_report, confusion_matrix, roc_auc_score\n",
    ")\n",
    "\n",
    "# Hyperparameter tuning\n",
    "from sklearn.model_selection import GridSearchCV, RandomizedSearchCV\n",
    "\n",
    "# Utilities\n",
    "import warnings\n",
    "warnings.filterwarnings('ignore')\n",
    "\n",
    "print(\"Libraries imported successfully!\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "fd65eebb",
   "metadata": {},
   "source": [
    "## 2. Load and Prepare Data"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "ffa2af23",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Load the datasets\n",
    "train_data = pd.read_csv('../data/raw/train_data.csv')\n",
    "train_labels = pd.read_csv('../data/raw/train_labels.csv')\n",
    "test_data = pd.read_csv('../data/raw/test_data.csv')\n",
    "\n",
    "print(f\"Training data shape: {train_data.shape}\")\n",
    "print(f\"Training labels shape: {train_labels.shape}\")\n",
    "print(f\"Test data shape: {test_data.shape}\")\n",
    "\n",
    "# Get target column\n",
    "target_column = train_labels.columns[-1]\n",
    "print(f\"Target column: {target_column}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "88f197a6",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Load problematic features from EDA\n",
    "try:\n",
    "    with open('../data/processed/problematic_features.json', 'r') as f:\n",
    "        problematic_features = json.load(f)\n",
    "    \n",
    "    print(\"Problematic features loaded:\")\n",
    "    print(f\"- Constant features: {len(problematic_features['constant_features'])}\")\n",
    "    print(f\"- Low variance features: {len(problematic_features['low_variance_features'])}\")\n",
    "    print(f\"- Highly correlated pairs: {len(problematic_features['highly_correlated_features'])}\")\n",
    "except FileNotFoundError:\n",
    "    print(\"No problematic features file found. Will identify them here.\")\n",
    "    problematic_features = {'constant_features': [], 'low_variance_features': []}"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "5f66872d",
   "metadata": {},
   "source": [
    "## 3. Data Preprocessing"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "40cf0bae",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Remove problematic features\n",
    "features_to_remove = set(\n",
    "    problematic_features.get('constant_features', []) + \n",
    "    problematic_features.get('low_variance_features', [])\n",
    ")\n",
    "\n",
    "print(f\"Removing {len(features_to_remove)} problematic features\")\n",
    "\n",
    "# Keep only good features\n",
    "good_features = [col for col in train_data.columns if col not in features_to_remove]\n",
    "X = train_data[good_features].copy()\n",
    "y = train_labels[target_column].copy()\n",
    "\n",
    "print(f\"Features after cleaning: {X.shape[1]}\")\n",
    "print(f\"Target distribution: {y.value_counts().to_dict()}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "8f8c94b5",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Handle missing values if any\n",
    "if X.isnull().sum().sum() > 0:\n",
    "    print(\"Handling missing values...\")\n",
    "    X = X.fillna(X.median())\n",
    "\n",
    "# Encode target if needed\n",
    "le = LabelEncoder()\n",
    "if y.dtype == 'object':\n",
    "    y_encoded = le.fit_transform(y)\n",
    "    print(f\"Target encoded: {dict(zip(le.classes_, range(len(le.classes_))))}\")\n",
    "else:\n",
    "    y_encoded = y.values\n",
    "\n",
    "print(f\"Final data shape: {X.shape}\")\n",
    "print(f\"Target shape: {y_encoded.shape}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "9822a7dc",
   "metadata": {},
   "source": [
    "## 4. Train-Validation Split"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "34608926",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Split data\n",
    "X_train, X_val, y_train, y_val = train_test_split(\n",
    "    X, y_encoded, test_size=0.2, random_state=42, stratify=y_encoded\n",
    ")\n",
    "\n",
    "print(f\"Training set: {X_train.shape}\")\n",
    "print(f\"Validation set: {X_val.shape}\")\n",
    "print(f\"Training target distribution: {np.bincount(y_train)}\")\n",
    "print(f\"Validation target distribution: {np.bincount(y_val)}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "efddce08",
   "metadata": {},
   "source": [
    "## 5. Feature Scaling"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "322d77da",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Scale features\n",
    "scaler = StandardScaler()\n",
    "X_train_scaled = scaler.fit_transform(X_train)\n",
    "X_val_scaled = scaler.transform(X_val)\n",
    "\n",
    "print(f\"Scaled training features shape: {X_train_scaled.shape}\")\n",
    "print(f\"Feature scaling completed.\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "e25bd362",
   "metadata": {},
   "source": [
    "## 6. Feature Selection"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "cc73e5a3",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Select top K features\n",
    "k_features = min(1000, X_train_scaled.shape[1])  # Select top 1000 or all if less\n",
    "selector = SelectKBest(score_func=f_classif, k=k_features)\n",
    "X_train_selected = selector.fit_transform(X_train_scaled, y_train)\n",
    "X_val_selected = selector.transform(X_val_scaled)\n",
    "\n",
    "print(f\"Selected {k_features} features out of {X_train_scaled.shape[1]}\")\n",
    "print(f\"Selected features shape: {X_train_selected.shape}\")\n",
    "\n",
    "# Get feature scores\n",
    "feature_scores = selector.scores_\n",
    "selected_features = X.columns[selector.get_support()]\n",
    "print(f\"Top 10 features by F-score: {selected_features[:10].tolist()}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "27e5d8ba",
   "metadata": {},
   "source": [
    "## 7. Model Comparison"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "257be39e",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Define models to compare\n",
    "models = {\n",
    "    'Logistic Regression': LogisticRegression(random_state=42, max_iter=1000),\n",
    "    'Random Forest': RandomForestClassifier(random_state=42, n_estimators=100),\n",
    "    'Gradient Boosting': GradientBoostingClassifier(random_state=42),\n",
    "    'SVM': SVC(random_state=42, probability=True),\n",
    "    'KNN': KNeighborsClassifier(),\n",
    "    'Naive Bayes': GaussianNB(),\n",
    "    'Decision Tree': DecisionTreeClassifier(random_state=42),\n",
    "    'XGBoost': xgb.XGBClassifier(random_state=42, eval_metric='logloss'),\n",
    "    'LightGBM': lgb.LGBMClassifier(random_state=42, verbose=-1)\n",
    "}\n",
    "\n",
    "print(f\"Will compare {len(models)} different models\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "040ce75e",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Cross-validation comparison\n",
    "cv_results = {}\n",
    "cv_folds = StratifiedKFold(n_splits=5, shuffle=True, random_state=42)\n",
    "\n",
    "print(\"Performing cross-validation for model comparison...\")\n",
    "\n",
    "for name, model in models.items():\n",
    "    print(f\"Training {name}...\")\n",
    "    \n",
    "    # Perform cross-validation\n",
    "    cv_scores = cross_val_score(\n",
    "        model, X_train_selected, y_train, \n",
    "        cv=cv_folds, scoring='accuracy', n_jobs=-1\n",
    "    )\n",
    "    \n",
    "    cv_results[name] = {\n",
    "        'mean_score': cv_scores.mean(),\n",
    "        'std_score': cv_scores.std(),\n",
    "        'scores': cv_scores\n",
    "    }\n",
    "    \n",
    "    print(f\"{name}: {cv_scores.mean():.4f} (+/- {cv_scores.std() * 2:.4f})\")\n",
    "\n",
    "print(\"\\nCross-validation completed!\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "a5d02989",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Visualize CV results\n",
    "model_names = list(cv_results.keys())\n",
    "mean_scores = [cv_results[name]['mean_score'] for name in model_names]\n",
    "std_scores = [cv_results[name]['std_score'] for name in model_names]\n",
    "\n",
    "# Create bar plot\n",
    "plt.figure(figsize=(12, 8))\n",
    "bars = plt.bar(model_names, mean_scores, yerr=std_scores, capsize=5, alpha=0.7)\n",
    "plt.title('Model Comparison - Cross Validation Accuracy')\n",
    "plt.xlabel('Models')\n",
    "plt.ylabel('Accuracy')\n",
    "plt.xticks(rotation=45, ha='right')\n",
    "plt.grid(axis='y', alpha=0.3)\n",
    "\n",
    "# Add value labels on bars\n",
    "for bar, score in zip(bars, mean_scores):\n",
    "    plt.text(bar.get_x() + bar.get_width()/2, bar.get_height() + 0.001, \n",
    "             f'{score:.3f}', ha='center', va='bottom')\n",
    "\n",
    "plt.tight_layout()\n",
    "plt.show()\n",
    "\n",
    "# Sort results\n",
    "sorted_results = sorted(cv_results.items(), key=lambda x: x[1]['mean_score'], reverse=True)\n",
    "print(\"\\nModel ranking by CV accuracy:\")\n",
    "for i, (name, results) in enumerate(sorted_results, 1):\n",
    "    print(f\"{i}. {name}: {results['mean_score']:.4f} (+/- {results['std_score'] * 2:.4f})\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "f7b065db",
   "metadata": {},
   "source": [
    "## 8. Detailed Evaluation of Top Models"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "9fcd6c16",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Select top 3 models for detailed evaluation\n",
    "top_models = dict(sorted_results[:3])\n",
    "print(f\"Top 3 models for detailed evaluation: {list(top_models.keys())}\")\n",
    "\n",
    "# Train and evaluate top models\n",
    "detailed_results = {}\n",
    "\n",
    "for name, _ in top_models.items():\n",
    "    print(f\"\\nDetailed evaluation of {name}:\")\n",
    "    \n",
    "    # Get model\n",
    "    model = models[name]\n",
    "    \n",
    "    # Train model\n",
    "    model.fit(X_train_selected, y_train)\n",
    "    \n",
    "    # Predictions\n",
    "    y_pred = model.predict(X_val_selected)\n",
    "    y_pred_proba = model.predict_proba(X_val_selected) if hasattr(model, 'predict_proba') else None\n",
    "    \n",
    "    # Calculate metrics\n",
    "    accuracy = accuracy_score(y_val, y_pred)\n",
    "    precision = precision_score(y_val, y_pred, average='weighted')\n",
    "    recall = recall_score(y_val, y_pred, average='weighted')\n",
    "    f1 = f1_score(y_val, y_pred, average='weighted')\n",
    "    \n",
    "    detailed_results[name] = {\n",
    "        'model': model,\n",
    "        'accuracy': accuracy,\n",
    "        'precision': precision,\n",
    "        'recall': recall,\n",
    "        'f1': f1,\n",
    "        'predictions': y_pred,\n",
    "        'probabilities': y_pred_proba\n",
    "    }\n",
    "    \n",
    "    print(f\"  Accuracy: {accuracy:.4f}\")\n",
    "    print(f\"  Precision: {precision:.4f}\")\n",
    "    print(f\"  Recall: {recall:.4f}\")\n",
    "    print(f\"  F1-Score: {f1:.4f}\")\n",
    "    \n",
    "    # Classification report\n",
    "    print(f\"\\n  Classification Report:\")\n",
    "    print(classification_report(y_val, y_pred, target_names=[f'Class_{i}' for i in np.unique(y_val)]))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "845096b0",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Confusion matrices for top models\n",
    "fig, axes = plt.subplots(1, len(top_models), figsize=(15, 4))\n",
    "if len(top_models) == 1:\n",
    "    axes = [axes]\n",
    "\n",
    "for idx, (name, results) in enumerate(detailed_results.items()):\n",
    "    cm = confusion_matrix(y_val, results['predictions'])\n",
    "    \n",
    "    sns.heatmap(cm, annot=True, fmt='d', ax=axes[idx], cmap='Blues')\n",
    "    axes[idx].set_title(f'{name}\\nAccuracy: {results[\"accuracy\"]:.3f}')\n",
    "    axes[idx].set_xlabel('Predicted')\n",
    "    axes[idx].set_ylabel('Actual')\n",
    "\n",
    "plt.tight_layout()\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "2a8a0fd3",
   "metadata": {},
   "source": [
    "## 9. Hyperparameter Tuning for Best Model"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "5254bc21",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Select best model based on validation performance\n",
    "best_model_name = max(detailed_results.keys(), \n",
    "                     key=lambda x: detailed_results[x]['accuracy'])\n",
    "print(f\"Best model for hyperparameter tuning: {best_model_name}\")\n",
    "\n",
    "# Define hyperparameter grids\n",
    "param_grids = {\n",
    "    'Random Forest': {\n",
    "        'n_estimators': [100, 200, 300],\n",
    "        'max_depth': [10, 20, None],\n",
    "        'min_samples_split': [2, 5, 10],\n",
    "        'min_samples_leaf': [1, 2, 4]\n",
    "    },\n",
    "    'XGBoost': {\n",
    "        'n_estimators': [100, 200, 300],\n",
    "        'max_depth': [3, 6, 10],\n",
    "        'learning_rate': [0.01, 0.1, 0.2],\n",
    "        'subsample': [0.8, 0.9, 1.0]\n",
    "    },\n",
    "    'LightGBM': {\n",
    "        'n_estimators': [100, 200, 300],\n",
    "        'max_depth': [3, 6, 10],\n",
    "        'learning_rate': [0.01, 0.1, 0.2],\n",
    "        'num_leaves': [31, 50, 100]\n",
    "    },\n",
    "    'Logistic Regression': {\n",
    "        'C': [0.1, 1, 10, 100],\n",
    "        'penalty': ['l1', 'l2'],\n",
    "        'solver': ['liblinear', 'lbfgs']\n",
    "    }\n",
    "}\n",
    "\n",
    "# Get appropriate parameter grid\n",
    "if best_model_name in param_grids:\n",
    "    param_grid = param_grids[best_model_name]\n",
    "    \n",
    "    print(f\"Tuning hyperparameters for {best_model_name}...\")\n",
    "    print(f\"Parameter grid: {param_grid}\")\n",
    "    \n",
    "    # Create fresh model instance\n",
    "    base_model = models[best_model_name]\n",
    "    \n",
    "    # Randomized search for efficiency\n",
    "    random_search = RandomizedSearchCV(\n",
    "        base_model, param_grid, n_iter=20, cv=3,\n",
    "        scoring='accuracy', random_state=42, n_jobs=-1\n",
    "    )\n",
    "    \n",
    "    # Fit the search\n",
    "    random_search.fit(X_train_selected, y_train)\n",
    "    \n",
    "    print(f\"Best parameters: {random_search.best_params_}\")\n",
    "    print(f\"Best CV score: {random_search.best_score_:.4f}\")\n",
    "    \n",
    "    # Get best model\n",
    "    best_tuned_model = random_search.best_estimator_\n",
    "    \n",
    "else:\n",
    "    print(f\"No hyperparameter grid defined for {best_model_name}\")\n",
    "    best_tuned_model = detailed_results[best_model_name]['model']"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "16ea263d",
   "metadata": {},
   "source": [
    "## 10. Final Model Evaluation"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "2171a102",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Evaluate tuned model on validation set\n",
    "final_predictions = best_tuned_model.predict(X_val_selected)\n",
    "final_probabilities = best_tuned_model.predict_proba(X_val_selected)\n",
    "\n",
    "# Calculate final metrics\n",
    "final_accuracy = accuracy_score(y_val, final_predictions)\n",
    "final_precision = precision_score(y_val, final_predictions, average='weighted')\n",
    "final_recall = recall_score(y_val, final_predictions, average='weighted')\n",
    "final_f1 = f1_score(y_val, final_predictions, average='weighted')\n",
    "\n",
    "print(\"=== FINAL MODEL PERFORMANCE ===\")\n",
    "print(f\"Model: {best_model_name} (Tuned)\")\n",
    "print(f\"Accuracy: {final_accuracy:.4f}\")\n",
    "print(f\"Precision: {final_precision:.4f}\")\n",
    "print(f\"Recall: {final_recall:.4f}\")\n",
    "print(f\"F1-Score: {final_f1:.4f}\")\n",
    "\n",
    "print(\"\\nDetailed Classification Report:\")\n",
    "print(classification_report(y_val, final_predictions, \n",
    "                          target_names=[f'Class_{i}' for i in np.unique(y_val)]))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "11d01c53",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Feature importance for tree-based models\n",
    "if hasattr(best_tuned_model, 'feature_importances_'):\n",
    "    feature_importance = pd.DataFrame({\n",
    "        'feature': selected_features,\n",
    "        'importance': best_tuned_model.feature_importances_\n",
    "    }).sort_values('importance', ascending=False)\n",
    "    \n",
    "    # Plot top 20 important features\n",
    "    plt.figure(figsize=(12, 8))\n",
    "    top_features = feature_importance.head(20)\n",
    "    plt.barh(range(len(top_features)), top_features['importance'])\n",
    "    plt.yticks(range(len(top_features)), top_features['feature'])\n",
    "    plt.xlabel('Feature Importance')\n",
    "    plt.title(f'Top 20 Feature Importances - {best_model_name}')\n",
    "    plt.gca().invert_yaxis()\n",
    "    plt.tight_layout()\n",
    "    plt.show()\n",
    "    \n",
    "    print(f\"Top 10 most important features:\")\n",
    "    print(feature_importance.head(10))\n",
    "else:\n",
    "    print(f\"Feature importance not available for {best_model_name}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "41501b8f",
   "metadata": {},
   "source": [
    "## 11. Model Persistence"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "06c24826",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Save the best model and preprocessing objects\n",
    "import joblib\n",
    "import os\n",
    "\n",
    "# Create models directory if it doesn't exist\n",
    "os.makedirs('../models', exist_ok=True)\n",
    "\n",
    "# Save model\n",
    "joblib.dump(best_tuned_model, f'../models/best_model_{best_model_name.lower().replace(\" \", \"_\")}.pkl')\n",
    "\n",
    "# Save preprocessing objects\n",
    "joblib.dump(scaler, '../models/scaler.pkl')\n",
    "joblib.dump(selector, '../models/feature_selector.pkl')\n",
    "\n",
    "# Save label encoder if used\n",
    "if 'le' in locals() and hasattr(le, 'classes_'):\n",
    "    joblib.dump(le, '../models/label_encoder.pkl')\n",
    "\n",
    "# Save model metadata\n",
    "model_metadata = {\n",
    "    'model_name': best_model_name,\n",
    "    'model_type': type(best_tuned_model).__name__,\n",
    "    'features_used': len(selected_features),\n",
    "    'total_features': X.shape[1],\n",
    "    'validation_accuracy': final_accuracy,\n",
    "    'validation_f1': final_f1,\n",
    "    'hyperparameters': best_tuned_model.get_params(),\n",
    "    'selected_features': selected_features.tolist(),\n",
    "    'training_date': pd.Timestamp.now().isoformat()\n",
    "}\n",
    "\n",
    "with open('../models/model_metadata.json', 'w') as f:\n",
    "    json.dump(model_metadata, f, indent=2)\n",
    "\n",
    "print(\"Model and preprocessing objects saved to ../models/ directory\")\n",
    "print(f\"Files saved:\")\n",
    "print(f\"- best_model_{best_model_name.lower().replace(' ', '_')}.pkl\")\n",
    "print(f\"- scaler.pkl\")\n",
    "print(f\"- feature_selector.pkl\")\n",
    "print(f\"- model_metadata.json\")\n",
    "if 'le' in locals() and hasattr(le, 'classes_'):\n",
    "    print(f\"- label_encoder.pkl\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "ff84fac9",
   "metadata": {},
   "source": [
    "## 12. Model Comparison Summary"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "292abb85",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Create comprehensive comparison table\n",
    "comparison_df = pd.DataFrame({\n",
    "    'Model': list(cv_results.keys()),\n",
    "    'CV_Mean': [cv_results[name]['mean_score'] for name in cv_results.keys()],\n",
    "    'CV_Std': [cv_results[name]['std_score'] for name in cv_results.keys()]\n",
    "})\n",
    "\n",
    "# Add detailed metrics for top models\n",
    "for name in detailed_results.keys():\n",
    "    idx = comparison_df[comparison_df['Model'] == name].index[0]\n",
    "    comparison_df.loc[idx, 'Val_Accuracy'] = detailed_results[name]['accuracy']\n",
    "    comparison_df.loc[idx, 'Val_Precision'] = detailed_results[name]['precision']\n",
    "    comparison_df.loc[idx, 'Val_Recall'] = detailed_results[name]['recall']\n",
    "    comparison_df.loc[idx, 'Val_F1'] = detailed_results[name]['f1']\n",
    "\n",
    "# Sort by CV score\n",
    "comparison_df = comparison_df.sort_values('CV_Mean', ascending=False)\n",
    "\n",
    "print(\"=== MODEL COMPARISON SUMMARY ===\")\n",
    "display(comparison_df.round(4))\n",
    "\n",
    "print(f\"\\n=== SELECTED MODEL ===\")\n",
    "print(f\"Best Model: {best_model_name}\")\n",
    "print(f\"Validation Accuracy: {final_accuracy:.4f}\")\n",
    "print(f\"Validation F1-Score: {final_f1:.4f}\")\n",
    "print(f\"Features Used: {len(selected_features)} out of {X.shape[1]}\")\n",
    "\n",
    "print(f\"\\n=== NEXT STEPS ===\")\n",
    "print(\"1. Use the selected model for final training on full dataset\")\n",
    "print(\"2. Generate predictions on test set\")\n",
    "print(\"3. Consider ensemble methods if needed\")\n",
    "print(\"4. Validate model performance on unseen data\")"
   ]
  }
 ],
 "metadata": {
  "language_info": {
   "name": "python"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
