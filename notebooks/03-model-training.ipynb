{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "26532388",
   "metadata": {},
   "source": [
    "# Final Model Training and Evaluation\n",
    "\n",
    "This notebook performs final model training using the best model selected from model comparison.\n",
    "\n",
    "## Objectives:\n",
    "- Load the best model configuration\n",
    "- Train on the full training dataset\n",
    "- Evaluate on test set\n",
    "- Generate final predictions\n",
    "- Create model performance report\n",
    "- Save final model artifacts"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "c9522a7a",
   "metadata": {},
   "source": [
    "## 1. Import Required Libraries"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "631c877e",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Data manipulation\n",
    "import pandas as pd\n",
    "import numpy as np\n",
    "import json\n",
    "import joblib\n",
    "import os\n",
    "from datetime import datetime\n",
    "\n",
    "# Visualization\n",
    "import matplotlib.pyplot as plt\n",
    "import seaborn as sns\n",
    "import plotly.express as px\n",
    "import plotly.graph_objects as go\n",
    "\n",
    "# Machine Learning\n",
    "from sklearn.preprocessing import StandardScaler, LabelEncoder\n",
    "from sklearn.feature_selection import SelectKBest, f_classif\n",
    "from sklearn.model_selection import learning_curve, validation_curve\n",
    "from sklearn.metrics import (\n",
    "    accuracy_score, precision_score, recall_score, f1_score,\n",
    "    classification_report, confusion_matrix, roc_auc_score,\n",
    "    roc_curve, precision_recall_curve\n",
    ")\n",
    "\n",
    "# Model imports (will be loaded based on metadata)\n",
    "from sklearn.ensemble import RandomForestClassifier, GradientBoostingClassifier\n",
    "from sklearn.linear_model import LogisticRegression\n",
    "from sklearn.svm import SVC\n",
    "import xgboost as xgb\n",
    "import lightgbm as lgb\n",
    "\n",
    "# Utilities\n",
    "import warnings\n",
    "warnings.filterwarnings('ignore')\n",
    "\n",
    "print(\"Libraries imported successfully!\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "60461dac",
   "metadata": {},
   "source": [
    "## 2. Load Data and Previous Results"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "30ac5276",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Load datasets\n",
    "train_data = pd.read_csv('../data/raw/train_data.csv')\n",
    "train_labels = pd.read_csv('../data/raw/train_labels.csv')\n",
    "test_data = pd.read_csv('../data/raw/test_data.csv')\n",
    "\n",
    "print(f\"Training data shape: {train_data.shape}\")\n",
    "print(f\"Training labels shape: {train_labels.shape}\")\n",
    "print(f\"Test data shape: {test_data.shape}\")\n",
    "\n",
    "# Get target column\n",
    "target_column = train_labels.columns[-1]\n",
    "print(f\"Target column: {target_column}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "99eb1129",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Load model metadata from previous selection\n",
    "try:\n",
    "    with open('../models/model_metadata.json', 'r') as f:\n",
    "        model_metadata = json.load(f)\n",
    "    \n",
    "    print(\"Loaded model metadata:\")\n",
    "    print(f\"- Best model: {model_metadata['model_name']}\")\n",
    "    print(f\"- Model type: {model_metadata['model_type']}\")\n",
    "    print(f\"- Validation accuracy: {model_metadata['validation_accuracy']:.4f}\")\n",
    "    print(f\"- Features used: {model_metadata['features_used']}\")\n",
    "    \n",
    "except FileNotFoundError:\n",
    "    print(\"No model metadata found. Please run model selection notebook first.\")\n",
    "    model_metadata = None"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "a36b9315",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Load problematic features if available\n",
    "try:\n",
    "    with open('../data/processed/problematic_features.json', 'r') as f:\n",
    "        problematic_features = json.load(f)\n",
    "    print(f\"Loaded problematic features info\")\n",
    "except FileNotFoundError:\n",
    "    print(\"No problematic features file found\")\n",
    "    problematic_features = {'constant_features': [], 'low_variance_features': []}"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "50abd830",
   "metadata": {},
   "source": [
    "## 3. Data Preprocessing (Consistent with Model Selection)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "4fdaa059",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Apply same preprocessing as in model selection\n",
    "features_to_remove = set(\n",
    "    problematic_features.get('constant_features', []) + \n",
    "    problematic_features.get('low_variance_features', [])\n",
    ")\n",
    "\n",
    "# Prepare training data\n",
    "good_features = [col for col in train_data.columns if col not in features_to_remove]\n",
    "X_full = train_data[good_features].copy()\n",
    "y_full = train_labels[target_column].copy()\n",
    "\n",
    "# Prepare test data\n",
    "X_test = test_data[good_features].copy()\n",
    "\n",
    "print(f\"Features after cleaning: {X_full.shape[1]}\")\n",
    "print(f\"Training samples: {X_full.shape[0]}\")\n",
    "print(f\"Test samples: {X_test.shape[0]}\")\n",
    "\n",
    "# Handle missing values\n",
    "if X_full.isnull().sum().sum() > 0:\n",
    "    print(\"Filling missing values...\")\n",
    "    X_full = X_full.fillna(X_full.median())\n",
    "    X_test = X_test.fillna(X_full.median())  # Use training median for test\n",
    "\n",
    "# Encode target if needed\n",
    "le = LabelEncoder()\n",
    "if y_full.dtype == 'object':\n",
    "    y_encoded = le.fit_transform(y_full)\n",
    "    print(f\"Target encoded: {dict(zip(le.classes_, range(len(le.classes_))))}\")\n",
    "else:\n",
    "    y_encoded = y_full.values\n",
    "    le = None"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "ee350710",
   "metadata": {},
   "source": [
    "## 4. Feature Scaling and Selection"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "474f27a5",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Scale features\n",
    "scaler = StandardScaler()\n",
    "X_scaled = scaler.fit_transform(X_full)\n",
    "X_test_scaled = scaler.transform(X_test)\n",
    "\n",
    "print(f\"Feature scaling completed\")\n",
    "\n",
    "# Feature selection (use same number as in model selection)\n",
    "if model_metadata:\n",
    "    k_features = model_metadata['features_used']\n",
    "else:\n",
    "    k_features = min(1000, X_scaled.shape[1])\n",
    "\n",
    "selector = SelectKBest(score_func=f_classif, k=k_features)\n",
    "X_selected = selector.fit_transform(X_scaled, y_encoded)\n",
    "X_test_selected = selector.transform(X_test_scaled)\n",
    "\n",
    "selected_features = X_full.columns[selector.get_support()]\n",
    "print(f\"Selected {k_features} features for training\")\n",
    "print(f\"Final training shape: {X_selected.shape}\")\n",
    "print(f\"Final test shape: {X_test_selected.shape}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "adb04a61",
   "metadata": {},
   "source": [
    "## 5. Load or Create Final Model"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "ee768876",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Try to load the best model from model selection\n",
    "if model_metadata:\n",
    "    model_name = model_metadata['model_name']\n",
    "    model_file = f\"../models/best_model_{model_name.lower().replace(' ', '_')}.pkl\"\n",
    "    \n",
    "    try:\n",
    "        best_model = joblib.load(model_file)\n",
    "        print(f\"Loaded saved model: {model_name}\")\n",
    "        print(f\"Model parameters: {best_model.get_params()}\")\n",
    "        \n",
    "    except FileNotFoundError:\n",
    "        print(f\"Saved model not found. Creating new {model_name} with best parameters.\")\n",
    "        \n",
    "        # Create model with best hyperparameters from metadata\n",
    "        if model_metadata['model_type'] == 'RandomForestClassifier':\n",
    "            best_model = RandomForestClassifier(**model_metadata['hyperparameters'])\n",
    "        elif model_metadata['model_type'] == 'XGBClassifier':\n",
    "            best_model = xgb.XGBClassifier(**model_metadata['hyperparameters'])\n",
    "        elif model_metadata['model_type'] == 'LGBMClassifier':\n",
    "            best_model = lgb.LGBMClassifier(**model_metadata['hyperparameters'])\n",
    "        elif model_metadata['model_type'] == 'LogisticRegression':\n",
    "            best_model = LogisticRegression(**model_metadata['hyperparameters'])\n",
    "        else:\n",
    "            print(f\"Unknown model type: {model_metadata['model_type']}\")\n",
    "            best_model = RandomForestClassifier(random_state=42)\n",
    "            \n",
    "else:\n",
    "    print(\"No model metadata available. Using default RandomForest.\")\n",
    "    best_model = RandomForestClassifier(n_estimators=200, random_state=42)\n",
    "    model_name = \"Random Forest (Default)\""
   ]
  },
  {
   "cell_type": "markdown",
   "id": "93e16b14",
   "metadata": {},
   "source": [
    "## 6. Final Model Training"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "261034e9",
   "metadata": {},
   "outputs": [],
   "source": [
    "print(f\"Training final model: {type(best_model).__name__}\")\n",
    "print(f\"Training on {X_selected.shape[0]} samples with {X_selected.shape[1]} features\")\n",
    "\n",
    "# Train the model\n",
    "start_time = datetime.now()\n",
    "best_model.fit(X_selected, y_encoded)\n",
    "training_time = datetime.now() - start_time\n",
    "\n",
    "print(f\"Training completed in: {training_time}\")\n",
    "print(f\"Model trained successfully!\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "add6c5e2",
   "metadata": {},
   "source": [
    "## 7. Learning Curves Analysis"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "cd55222e",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Generate learning curves\n",
    "print(\"Generating learning curves...\")\n",
    "\n",
    "train_sizes = np.linspace(0.1, 1.0, 10)\n",
    "train_sizes_abs, train_scores, val_scores = learning_curve(\n",
    "    best_model, X_selected, y_encoded, \n",
    "    train_sizes=train_sizes, cv=5, scoring='accuracy',\n",
    "    random_state=42, n_jobs=-1\n",
    ")\n",
    "\n",
    "# Calculate means and stds\n",
    "train_mean = np.mean(train_scores, axis=1)\n",
    "train_std = np.std(train_scores, axis=1)\n",
    "val_mean = np.mean(val_scores, axis=1)\n",
    "val_std = np.std(val_scores, axis=1)\n",
    "\n",
    "# Plot learning curves\n",
    "plt.figure(figsize=(12, 8))\n",
    "plt.plot(train_sizes_abs, train_mean, 'o-', color='blue', label='Training Accuracy')\n",
    "plt.fill_between(train_sizes_abs, train_mean - train_std, train_mean + train_std, alpha=0.1, color='blue')\n",
    "\n",
    "plt.plot(train_sizes_abs, val_mean, 'o-', color='red', label='Validation Accuracy')\n",
    "plt.fill_between(train_sizes_abs, val_mean - val_std, val_mean + val_std, alpha=0.1, color='red')\n",
    "\n",
    "plt.xlabel('Training Set Size')\n",
    "plt.ylabel('Accuracy Score')\n",
    "plt.title(f'Learning Curves - {type(best_model).__name__}')\n",
    "plt.legend()\n",
    "plt.grid(True, alpha=0.3)\n",
    "plt.tight_layout()\n",
    "plt.show()\n",
    "\n",
    "print(f\"Final training accuracy: {train_mean[-1]:.4f} (+/- {train_std[-1]:.4f})\")\n",
    "print(f\"Final validation accuracy: {val_mean[-1]:.4f} (+/- {val_std[-1]:.4f})\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "89a97fec",
   "metadata": {},
   "source": [
    "## 8. Feature Importance Analysis"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "837adb0f",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Feature importance analysis\n",
    "if hasattr(best_model, 'feature_importances_'):\n",
    "    feature_importance = pd.DataFrame({\n",
    "        'feature': selected_features,\n",
    "        'importance': best_model.feature_importances_\n",
    "    }).sort_values('importance', ascending=False)\n",
    "    \n",
    "    # Save feature importance\n",
    "    feature_importance.to_csv('../data/processed/final_feature_importance.csv', index=False)\n",
    "    \n",
    "    # Plot top 20 features\n",
    "    plt.figure(figsize=(12, 10))\n",
    "    top_20_features = feature_importance.head(20)\n",
    "    plt.barh(range(len(top_20_features)), top_20_features['importance'])\n",
    "    plt.yticks(range(len(top_20_features)), top_20_features['feature'])\n",
    "    plt.xlabel('Feature Importance')\n",
    "    plt.title(f'Top 20 Feature Importances - {type(best_model).__name__}')\n",
    "    plt.gca().invert_yaxis()\n",
    "    plt.tight_layout()\n",
    "    plt.show()\n",
    "    \n",
    "    print(\"Top 10 most important features:\")\n",
    "    display(feature_importance.head(10))\n",
    "    \n",
    "    # Feature importance statistics\n",
    "    print(f\"\\nFeature importance statistics:\")\n",
    "    print(f\"Mean importance: {feature_importance['importance'].mean():.6f}\")\n",
    "    print(f\"Std importance: {feature_importance['importance'].std():.6f}\")\n",
    "    print(f\"Max importance: {feature_importance['importance'].max():.6f}\")\n",
    "    print(f\"Min importance: {feature_importance['importance'].min():.6f}\")\n",
    "    \n",
    "else:\n",
    "    print(f\"Feature importance not available for {type(best_model).__name__}\")\n",
    "    feature_importance = None"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "fb0af33f",
   "metadata": {},
   "source": [
    "## 9. Generate Test Predictions"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "460ce81d",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Generate predictions on test set\n",
    "print(\"Generating predictions on test set...\")\n",
    "\n",
    "# Predictions\n",
    "test_predictions = best_model.predict(X_test_selected)\n",
    "test_probabilities = best_model.predict_proba(X_test_selected)\n",
    "\n",
    "print(f\"Generated predictions for {len(test_predictions)} test samples\")\n",
    "\n",
    "# Convert back to original labels if necessary\n",
    "if le is not None:\n",
    "    test_predictions_original = le.inverse_transform(test_predictions)\n",
    "    print(f\"Converted predictions back to original labels\")\n",
    "else:\n",
    "    test_predictions_original = test_predictions\n",
    "\n",
    "# Create predictions DataFrame\n",
    "predictions_df = pd.DataFrame({\n",
    "    'sample_id': range(len(test_predictions)),\n",
    "    'predicted_class': test_predictions_original\n",
    "})\n",
    "\n",
    "# Add probability columns\n",
    "for i, class_name in enumerate(np.unique(y_encoded)):\n",
    "    if le is not None:\n",
    "        original_class = le.inverse_transform([class_name])[0]\n",
    "        predictions_df[f'prob_class_{original_class}'] = test_probabilities[:, i]\n",
    "    else:\n",
    "        predictions_df[f'prob_class_{class_name}'] = test_probabilities[:, i]\n",
    "\n",
    "print(\"Prediction distribution:\")\n",
    "print(predictions_df['predicted_class'].value_counts())\n",
    "\n",
    "display(predictions_df.head(10))"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "358bc0a6",
   "metadata": {},
   "source": [
    "## 10. Model Performance Analysis"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "41b54519",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Training set performance (for reference)\n",
    "train_predictions = best_model.predict(X_selected)\n",
    "train_accuracy = accuracy_score(y_encoded, train_predictions)\n",
    "train_f1 = f1_score(y_encoded, train_predictions, average='weighted')\n",
    "\n",
    "print(\"=== MODEL PERFORMANCE SUMMARY ===\")\n",
    "print(f\"Model: {type(best_model).__name__}\")\n",
    "print(f\"Training Samples: {X_selected.shape[0]}\")\n",
    "print(f\"Features Used: {X_selected.shape[1]}\")\n",
    "print(f\"Training Time: {training_time}\")\n",
    "print(f\"\\nTraining Performance:\")\n",
    "print(f\"  Accuracy: {train_accuracy:.4f}\")\n",
    "print(f\"  F1-Score: {train_f1:.4f}\")\n",
    "\n",
    "if model_metadata:\n",
    "    print(f\"\\nValidation Performance (from model selection):\")\n",
    "    print(f\"  Accuracy: {model_metadata['validation_accuracy']:.4f}\")\n",
    "    print(f\"  F1-Score: {model_metadata['validation_f1']:.4f}\")\n",
    "\n",
    "print(f\"\\nTest Predictions Generated: {len(test_predictions)}\")\n",
    "print(f\"Test Prediction Distribution: {dict(zip(*np.unique(test_predictions_original, return_counts=True)))}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "716fba02",
   "metadata": {},
   "source": [
    "## 11. Prediction Confidence Analysis"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "d692625d",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Analyze prediction confidence\n",
    "max_probabilities = np.max(test_probabilities, axis=1)\n",
    "confidence_stats = pd.Series(max_probabilities).describe()\n",
    "\n",
    "print(\"Prediction Confidence Statistics:\")\n",
    "print(confidence_stats)\n",
    "\n",
    "# Plot confidence distribution\n",
    "plt.figure(figsize=(12, 5))\n",
    "\n",
    "plt.subplot(1, 2, 1)\n",
    "plt.hist(max_probabilities, bins=30, alpha=0.7, color='skyblue', edgecolor='black')\n",
    "plt.xlabel('Maximum Probability')\n",
    "plt.ylabel('Frequency')\n",
    "plt.title('Distribution of Prediction Confidence')\n",
    "plt.axvline(confidence_stats['mean'], color='red', linestyle='--', label=f'Mean: {confidence_stats[\"mean\"]:.3f}')\n",
    "plt.legend()\n",
    "\n",
    "plt.subplot(1, 2, 2)\n",
    "plt.boxplot(max_probabilities)\n",
    "plt.ylabel('Maximum Probability')\n",
    "plt.title('Prediction Confidence Box Plot')\n",
    "\n",
    "plt.tight_layout()\n",
    "plt.show()\n",
    "\n",
    "# High and low confidence predictions\n",
    "high_confidence_threshold = 0.9\n",
    "low_confidence_threshold = 0.6\n",
    "\n",
    "high_confidence_count = np.sum(max_probabilities >= high_confidence_threshold)\n",
    "low_confidence_count = np.sum(max_probabilities <= low_confidence_threshold)\n",
    "\n",
    "print(f\"\\nConfidence Analysis:\")\n",
    "print(f\"High confidence predictions (â‰¥{high_confidence_threshold}): {high_confidence_count} ({high_confidence_count/len(max_probabilities)*100:.1f}%)\")\n",
    "print(f\"Low confidence predictions (â‰¤{low_confidence_threshold}): {low_confidence_count} ({low_confidence_count/len(max_probabilities)*100:.1f}%)\")\n",
    "print(f\"Medium confidence predictions: {len(max_probabilities) - high_confidence_count - low_confidence_count} ({(len(max_probabilities) - high_confidence_count - low_confidence_count)/len(max_probabilities)*100:.1f}%)\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "9de35753",
   "metadata": {},
   "source": [
    "## 12. Save Final Results"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "44f7c16e",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Save predictions\n",
    "predictions_df.to_csv('../data/processed/test_predictions.csv', index=False)\n",
    "print(\"Test predictions saved to ../data/processed/test_predictions.csv\")\n",
    "\n",
    "# Save final model and preprocessing objects\n",
    "model_filename = f\"final_model_{type(best_model).__name__.lower()}_{datetime.now().strftime('%Y%m%d_%H%M%S')}.pkl\"\n",
    "joblib.dump(best_model, f'../models/{model_filename}')\n",
    "joblib.dump(scaler, '../models/final_scaler.pkl')\n",
    "joblib.dump(selector, '../models/final_feature_selector.pkl')\n",
    "\n",
    "if le is not None:\n",
    "    joblib.dump(le, '../models/final_label_encoder.pkl')\n",
    "\n",
    "print(f\"Final model saved as: {model_filename}\")\n",
    "\n",
    "# Create comprehensive model report\n",
    "final_report = {\n",
    "    'model_info': {\n",
    "        'model_name': type(best_model).__name__,\n",
    "        'model_parameters': best_model.get_params(),\n",
    "        'training_date': datetime.now().isoformat(),\n",
    "        'training_time_seconds': training_time.total_seconds()\n",
    "    },\n",
    "    'data_info': {\n",
    "        'training_samples': X_selected.shape[0],\n",
    "        'features_used': X_selected.shape[1],\n",
    "        'total_original_features': X_full.shape[1],\n",
    "        'test_samples': X_test_selected.shape[0],\n",
    "        'target_classes': int(len(np.unique(y_encoded))),\n",
    "        'feature_selection_method': 'SelectKBest with f_classif'\n",
    "    },\n",
    "    'performance': {\n",
    "        'training_accuracy': float(train_accuracy),\n",
    "        'training_f1_weighted': float(train_f1),\n",
    "        'learning_curve_final_train': float(train_mean[-1]),\n",
    "        'learning_curve_final_val': float(val_mean[-1])\n",
    "    },\n",
    "    'predictions': {\n",
    "        'test_samples_predicted': int(len(test_predictions)),\n",
    "        'prediction_distribution': {str(k): int(v) for k, v in zip(*np.unique(test_predictions_original, return_counts=True))},\n",
    "        'confidence_stats': {\n",
    "            'mean_confidence': float(confidence_stats['mean']),\n",
    "            'std_confidence': float(confidence_stats['std']),\n",
    "            'min_confidence': float(confidence_stats['min']),\n",
    "            'max_confidence': float(confidence_stats['max']),\n",
    "            'high_confidence_count': int(high_confidence_count),\n",
    "            'low_confidence_count': int(low_confidence_count)\n",
    "        }\n",
    "    },\n",
    "    'files_created': {\n",
    "        'model': model_filename,\n",
    "        'scaler': 'final_scaler.pkl',\n",
    "        'feature_selector': 'final_feature_selector.pkl',\n",
    "        'label_encoder': 'final_label_encoder.pkl' if le is not None else None,\n",
    "        'predictions': 'test_predictions.csv',\n",
    "        'feature_importance': 'final_feature_importance.csv' if feature_importance is not None else None\n",
    "    }\n",
    "}\n",
    "\n",
    "# Add feature importance to report if available\n",
    "if feature_importance is not None:\n",
    "    final_report['feature_importance'] = {\n",
    "        'top_10_features': feature_importance.head(10).to_dict('records'),\n",
    "        'importance_stats': {\n",
    "            'mean': float(feature_importance['importance'].mean()),\n",
    "            'std': float(feature_importance['importance'].std()),\n",
    "            'max': float(feature_importance['importance'].max()),\n",
    "            'min': float(feature_importance['importance'].min())\n",
    "        }\n",
    "    }\n",
    "\n",
    "# Save final report\n",
    "with open('../models/final_model_report.json', 'w') as f:\n",
    "    json.dump(final_report, f, indent=2)\n",
    "\n",
    "print(\"Final model report saved to ../models/final_model_report.json\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "1dab7eda",
   "metadata": {},
   "source": [
    "## 13. Final Summary"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "3e2d07ce",
   "metadata": {},
   "outputs": [],
   "source": [
    "print(\"\" + \"=\"*60)\n",
    "print(\"FINAL MODEL TRAINING COMPLETED SUCCESSFULLY\")\n",
    "print(\"=\"*60)\n",
    "\n",
    "print(f\"\\nðŸ¤– Model Details:\")\n",
    "print(f\"   Type: {type(best_model).__name__}\")\n",
    "print(f\"   Training Time: {training_time}\")\n",
    "print(f\"   Features Used: {X_selected.shape[1]:,} out of {X_full.shape[1]:,}\")\n",
    "\n",
    "print(f\"\\nðŸ“Š Performance:\")\n",
    "print(f\"   Training Accuracy: {train_accuracy:.4f}\")\n",
    "print(f\"   Training F1-Score: {train_f1:.4f}\")\n",
    "if model_metadata:\n",
    "    print(f\"   Validation Accuracy: {model_metadata['validation_accuracy']:.4f}\")\n",
    "\n",
    "print(f\"\\nðŸŽ¯ Predictions:\")\n",
    "print(f\"   Test Samples: {len(test_predictions):,}\")\n",
    "print(f\"   Mean Confidence: {confidence_stats['mean']:.3f}\")\n",
    "print(f\"   High Confidence (â‰¥90%): {high_confidence_count:,} ({high_confidence_count/len(max_probabilities)*100:.1f}%)\")\n",
    "\n",
    "print(f\"\\nðŸ’¾ Files Created:\")\n",
    "print(f\"   ðŸ“ ../models/{model_filename}\")\n",
    "print(f\"   ðŸ“ ../models/final_scaler.pkl\")\n",
    "print(f\"   ðŸ“ ../models/final_feature_selector.pkl\")\n",
    "print(f\"   ðŸ“ ../models/final_model_report.json\")\n",
    "print(f\"   ðŸ“ ../data/processed/test_predictions.csv\")\n",
    "if feature_importance is not None:\n",
    "    print(f\"   ðŸ“ ../data/processed/final_feature_importance.csv\")\n",
    "\n",
    "print(f\"\\nâœ… Next Steps:\")\n",
    "print(f\"   1. Review prediction results and confidence scores\")\n",
    "print(f\"   2. Validate model performance on additional data if available\")\n",
    "print(f\"   3. Deploy model for production use\")\n",
    "print(f\"   4. Monitor model performance over time\")\n",
    "print(f\"   5. Consider retraining with new data\")\n",
    "\n",
    "print(\"\\n\" + \"=\"*60)\n",
    "print(\"ðŸŽ‰ PROJECT COMPLETED SUCCESSFULLY!\")\n",
    "print(\"=\"*60)"
   ]
  }
 ],
 "metadata": {
  "language_info": {
   "name": "python"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
