{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "0587e2a8",
   "metadata": {},
   "source": [
    "# Exploratory Data Analysis (EDA)\n",
    "\n",
    "This notebook performs comprehensive exploratory data analysis on the brain activity data for music classification.\n",
    "\n",
    "## Objectives:\n",
    "- Load and inspect the dataset\n",
    "- Understand data structure and quality\n",
    "- Visualize feature distributions\n",
    "- Analyze target variable distribution\n",
    "- Identify correlations and patterns\n",
    "- Detect outliers and missing values"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "b69571ab",
   "metadata": {},
   "source": [
    "## 1. Import Required Libraries"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "33e34d43",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Data manipulation and analysis\n",
    "import pandas as pd\n",
    "import numpy as np\n",
    "\n",
    "# Visualization\n",
    "import matplotlib.pyplot as plt\n",
    "import seaborn as sns\n",
    "import plotly.express as px\n",
    "import plotly.graph_objects as go\n",
    "from plotly.subplots import make_subplots\n",
    "\n",
    "# Statistical analysis\n",
    "from scipy import stats\n",
    "from sklearn.preprocessing import StandardScaler\n",
    "from sklearn.decomposition import PCA\n",
    "\n",
    "# Utilities\n",
    "import warnings\n",
    "warnings.filterwarnings('ignore')\n",
    "\n",
    "# Set display options\n",
    "pd.set_option('display.max_columns', None)\n",
    "plt.style.use('seaborn-v0_8')\n",
    "sns.set_palette(\"husl\")\n",
    "\n",
    "print(\"Libraries imported successfully!\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "436a5c14",
   "metadata": {},
   "source": [
    "## 2. Load Dataset"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "8b9b9b02",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Load the datasets\n",
    "train_data = pd.read_csv('../data/raw/train_data.csv')\n",
    "train_labels = pd.read_csv('../data/raw/train_labels.csv')\n",
    "test_data = pd.read_csv('../data/raw/test_data.csv')\n",
    "\n",
    "print(f\"Training data shape: {train_data.shape}\")\n",
    "print(f\"Training labels shape: {train_labels.shape}\")\n",
    "print(f\"Test data shape: {test_data.shape}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "0eb121d8",
   "metadata": {},
   "source": [
    "## 3. Basic Data Inspection"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "9db250aa",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Display first few rows of training data\n",
    "print(\"Training Data - First 5 rows:\")\n",
    "display(train_data.head())\n",
    "\n",
    "print(\"\\nTraining Labels - First 5 rows:\")\n",
    "display(train_labels.head())"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "4180b15b",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Data info and statistics\n",
    "print(\"Training Data Info:\")\n",
    "print(train_data.info())\n",
    "\n",
    "print(\"\\nBasic Statistics:\")\n",
    "display(train_data.describe())"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "c0e7bdee",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Check for missing values\n",
    "print(\"Missing values in training data:\")\n",
    "missing_train = train_data.isnull().sum().sort_values(ascending=False)\n",
    "print(missing_train[missing_train > 0])\n",
    "\n",
    "print(\"\\nMissing values in training labels:\")\n",
    "missing_labels = train_labels.isnull().sum()\n",
    "print(missing_labels[missing_labels > 0])\n",
    "\n",
    "print(\"\\nMissing values in test data:\")\n",
    "missing_test = test_data.isnull().sum().sort_values(ascending=False)\n",
    "print(missing_test[missing_test > 0])"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "1395d939",
   "metadata": {},
   "source": [
    "## 4. Target Variable Analysis"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "d79dbebc",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Analyze target variable distribution\n",
    "target_column = train_labels.columns[-1]  # Assuming last column is target\n",
    "print(f\"Target variable: {target_column}\")\n",
    "\n",
    "# Value counts\n",
    "target_counts = train_labels[target_column].value_counts()\n",
    "print(f\"\\nTarget distribution:\")\n",
    "print(target_counts)\n",
    "\n",
    "# Calculate percentages\n",
    "target_percentages = train_labels[target_column].value_counts(normalize=True) * 100\n",
    "print(f\"\\nTarget distribution (percentages):\")\n",
    "print(target_percentages)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "2cd7318d",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Visualize target distribution\n",
    "fig, axes = plt.subplots(1, 2, figsize=(15, 5))\n",
    "\n",
    "# Bar plot\n",
    "target_counts.plot(kind='bar', ax=axes[0], color='skyblue')\n",
    "axes[0].set_title('Target Variable Distribution')\n",
    "axes[0].set_xlabel('Classes')\n",
    "axes[0].set_ylabel('Count')\n",
    "axes[0].tick_params(axis='x', rotation=45)\n",
    "\n",
    "# Pie chart\n",
    "axes[1].pie(target_counts.values, labels=target_counts.index, autopct='%1.1f%%', startangle=90)\n",
    "axes[1].set_title('Target Variable Distribution (Pie Chart)')\n",
    "\n",
    "plt.tight_layout()\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "4a0bafbf",
   "metadata": {},
   "source": [
    "## 5. Feature Analysis"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "9ebbf4d0",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Feature statistics\n",
    "numeric_features = train_data.select_dtypes(include=[np.number]).columns\n",
    "print(f\"Number of numeric features: {len(numeric_features)}\")\n",
    "print(f\"Feature names: {list(numeric_features[:10])}...\")  # Show first 10 features\n",
    "\n",
    "# Check data types\n",
    "print(\"\\nData types:\")\n",
    "print(train_data.dtypes.value_counts())"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "f0014fef",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Feature distribution analysis\n",
    "# Select first 16 features for visualization\n",
    "features_to_plot = numeric_features[:16]\n",
    "\n",
    "fig, axes = plt.subplots(4, 4, figsize=(20, 16))\n",
    "axes = axes.ravel()\n",
    "\n",
    "for idx, feature in enumerate(features_to_plot):\n",
    "    axes[idx].hist(train_data[feature], bins=30, alpha=0.7, color='lightblue', edgecolor='black')\n",
    "    axes[idx].set_title(f'Distribution of {feature}')\n",
    "    axes[idx].set_xlabel(feature)\n",
    "    axes[idx].set_ylabel('Frequency')\n",
    "\n",
    "plt.tight_layout()\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "df32bb75",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Box plots to identify outliers\n",
    "fig, axes = plt.subplots(4, 4, figsize=(20, 16))\n",
    "axes = axes.ravel()\n",
    "\n",
    "for idx, feature in enumerate(features_to_plot):\n",
    "    axes[idx].boxplot(train_data[feature])\n",
    "    axes[idx].set_title(f'Box Plot of {feature}')\n",
    "    axes[idx].set_ylabel(feature)\n",
    "\n",
    "plt.tight_layout()\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "3241a04a",
   "metadata": {},
   "source": [
    "## 6. Correlation Analysis"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "691aa0d2",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Calculate correlation matrix for a subset of features\n",
    "correlation_features = numeric_features[:20]  # Use first 20 features\n",
    "correlation_matrix = train_data[correlation_features].corr()\n",
    "\n",
    "# Create correlation heatmap\n",
    "plt.figure(figsize=(15, 12))\n",
    "sns.heatmap(correlation_matrix, annot=False, cmap='coolwarm', center=0, \n",
    "            square=True, linewidths=0.5)\n",
    "plt.title('Feature Correlation Matrix')\n",
    "plt.tight_layout()\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "fbbf7751",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Find highly correlated features\n",
    "def find_highly_correlated_features(df, threshold=0.8):\n",
    "    corr_matrix = df.corr().abs()\n",
    "    upper_triangle = corr_matrix.where(\n",
    "        np.triu(np.ones(corr_matrix.shape), k=1).astype(bool)\n",
    "    )\n",
    "    \n",
    "    highly_correlated = [(column, row, upper_triangle.loc[row, column])\n",
    "                        for column in upper_triangle.columns\n",
    "                        for row in upper_triangle.index\n",
    "                        if upper_triangle.loc[row, column] > threshold]\n",
    "    \n",
    "    return highly_correlated\n",
    "\n",
    "high_corr_features = find_highly_correlated_features(train_data[correlation_features])\n",
    "print(f\"Highly correlated feature pairs (correlation > 0.8): {len(high_corr_features)}\")\n",
    "for pair in high_corr_features[:10]:  # Show first 10 pairs\n",
    "    print(f\"{pair[0]} - {pair[1]}: {pair[2]:.3f}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "ec3c0d9e",
   "metadata": {},
   "source": [
    "## 7. Feature vs Target Analysis"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "7b5a8a97",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Merge training data with labels for analysis\n",
    "train_combined = train_data.copy()\n",
    "train_combined['target'] = train_labels[target_column]\n",
    "\n",
    "# Analyze feature distributions by target class\n",
    "features_for_analysis = numeric_features[:8]  # Use first 8 features\n",
    "\n",
    "fig, axes = plt.subplots(2, 4, figsize=(20, 10))\n",
    "axes = axes.ravel()\n",
    "\n",
    "for idx, feature in enumerate(features_for_analysis):\n",
    "    for target_class in train_combined['target'].unique():\n",
    "        subset = train_combined[train_combined['target'] == target_class]\n",
    "        axes[idx].hist(subset[feature], alpha=0.6, label=f'Class {target_class}', bins=20)\n",
    "    \n",
    "    axes[idx].set_title(f'{feature} Distribution by Target')\n",
    "    axes[idx].set_xlabel(feature)\n",
    "    axes[idx].set_ylabel('Frequency')\n",
    "    axes[idx].legend()\n",
    "\n",
    "plt.tight_layout()\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "85cd4451",
   "metadata": {},
   "source": [
    "## 8. Dimensionality Reduction Visualization"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "43d837d6",
   "metadata": {},
   "outputs": [],
   "source": [
    "# PCA for visualization\n",
    "scaler = StandardScaler()\n",
    "scaled_features = scaler.fit_transform(train_data[numeric_features])\n",
    "\n",
    "# Apply PCA\n",
    "pca = PCA(n_components=2)\n",
    "pca_features = pca.fit_transform(scaled_features)\n",
    "\n",
    "# Create PCA DataFrame\n",
    "pca_df = pd.DataFrame(data=pca_features, columns=['PC1', 'PC2'])\n",
    "pca_df['target'] = train_labels[target_column]\n",
    "\n",
    "# Plot PCA\n",
    "plt.figure(figsize=(10, 8))\n",
    "for target_class in pca_df['target'].unique():\n",
    "    subset = pca_df[pca_df['target'] == target_class]\n",
    "    plt.scatter(subset['PC1'], subset['PC2'], alpha=0.6, label=f'Class {target_class}')\n",
    "\n",
    "plt.xlabel(f'PC1 ({pca.explained_variance_ratio_[0]:.2%} variance)')\n",
    "plt.ylabel(f'PC2 ({pca.explained_variance_ratio_[1]:.2%} variance)')\n",
    "plt.title('PCA Visualization of Brain Activity Data')\n",
    "plt.legend()\n",
    "plt.grid(True, alpha=0.3)\n",
    "plt.show()\n",
    "\n",
    "print(f\"Total variance explained by first 2 components: {sum(pca.explained_variance_ratio_):.2%}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "898b08c3",
   "metadata": {},
   "source": [
    "## 9. Statistical Tests"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "b88b88ea",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Perform statistical tests for feature importance\n",
    "from scipy.stats import f_oneway\n",
    "\n",
    "# ANOVA test for top features\n",
    "anova_results = []\n",
    "top_features = numeric_features[:20]  # Test top 20 features\n",
    "\n",
    "for feature in top_features:\n",
    "    groups = [train_combined[train_combined['target'] == target_class][feature] \n",
    "              for target_class in train_combined['target'].unique()]\n",
    "    \n",
    "    f_stat, p_value = f_oneway(*groups)\n",
    "    anova_results.append({\n",
    "        'feature': feature,\n",
    "        'f_statistic': f_stat,\n",
    "        'p_value': p_value\n",
    "    })\n",
    "\n",
    "# Create DataFrame and sort by p-value\n",
    "anova_df = pd.DataFrame(anova_results)\n",
    "anova_df = anova_df.sort_values('p_value')\n",
    "\n",
    "print(\"ANOVA Test Results (Top 10 most significant features):\")\n",
    "display(anova_df.head(10))"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "a04ca12f",
   "metadata": {},
   "source": [
    "## 10. Data Quality Assessment"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "e2acf806",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Check for constant features\n",
    "constant_features = []\n",
    "for feature in numeric_features:\n",
    "    if train_data[feature].nunique() == 1:\n",
    "        constant_features.append(feature)\n",
    "\n",
    "print(f\"Constant features: {len(constant_features)}\")\n",
    "if constant_features:\n",
    "    print(constant_features[:10])  # Show first 10\n",
    "\n",
    "# Check for near-zero variance features\n",
    "low_variance_features = []\n",
    "for feature in numeric_features:\n",
    "    if train_data[feature].var() < 0.01:\n",
    "        low_variance_features.append(feature)\n",
    "\n",
    "print(f\"\\nLow variance features (var < 0.01): {len(low_variance_features)}\")\n",
    "if low_variance_features:\n",
    "    print(low_variance_features[:10])  # Show first 10"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "200b2a79",
   "metadata": {},
   "source": [
    "## 11. Summary and Key Findings"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "bc7dbc16",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Summary statistics\n",
    "print(\"=== EXPLORATORY DATA ANALYSIS SUMMARY ===\")\n",
    "print(f\"\\nDataset Shape:\")\n",
    "print(f\"  Training data: {train_data.shape}\")\n",
    "print(f\"  Training labels: {train_labels.shape}\")\n",
    "print(f\"  Test data: {test_data.shape}\")\n",
    "\n",
    "print(f\"\\nData Quality:\")\n",
    "print(f\"  Missing values in training data: {train_data.isnull().sum().sum()}\")\n",
    "print(f\"  Missing values in labels: {train_labels.isnull().sum().sum()}\")\n",
    "print(f\"  Constant features: {len(constant_features)}\")\n",
    "print(f\"  Low variance features: {len(low_variance_features)}\")\n",
    "\n",
    "print(f\"\\nTarget Variable:\")\n",
    "print(f\"  Number of classes: {train_labels[target_column].nunique()}\")\n",
    "print(f\"  Class distribution: {dict(train_labels[target_column].value_counts())}\")\n",
    "print(f\"  Is balanced: {'Yes' if train_labels[target_column].value_counts().std() < train_labels[target_column].value_counts().mean() * 0.1 else 'No'}\")\n",
    "\n",
    "print(f\"\\nFeature Characteristics:\")\n",
    "print(f\"  Number of numeric features: {len(numeric_features)}\")\n",
    "print(f\"  Highly correlated pairs (>0.8): {len(high_corr_features)}\")\n",
    "print(f\"  PCA variance explained (2 components): {sum(pca.explained_variance_ratio_):.2%}\")\n",
    "\n",
    "print(\"\\n=== RECOMMENDATIONS FOR NEXT STEPS ===\")\n",
    "print(\"1. Consider removing constant and low variance features\")\n",
    "print(\"2. Handle highly correlated features (feature selection or PCA)\")\n",
    "print(\"3. Consider feature scaling for distance-based algorithms\")\n",
    "print(\"4. Investigate class imbalance if present\")\n",
    "print(\"5. Consider outlier detection and treatment\")\n",
    "print(\"6. Feature engineering based on domain knowledge\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "1ae08c0a",
   "metadata": {},
   "source": [
    "## 12. Save Preprocessed Data"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "4ea22dba",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Save analysis results for next notebooks\n",
    "# You can save intermediate results here\n",
    "\n",
    "# Example: Save feature importance ranking\n",
    "anova_df.to_csv('../data/processed/feature_importance_anova.csv', index=False)\n",
    "\n",
    "# Save list of problematic features\n",
    "problematic_features = {\n",
    "    'constant_features': constant_features,\n",
    "    'low_variance_features': low_variance_features,\n",
    "    'highly_correlated_features': [pair[:2] for pair in high_corr_features]\n",
    "}\n",
    "\n",
    "import json\n",
    "with open('../data/processed/problematic_features.json', 'w') as f:\n",
    "    json.dump(problematic_features, f, indent=2)\n",
    "\n",
    "print(\"Analysis results saved to data/processed/ directory\")"
   ]
  }
 ],
 "metadata": {
  "language_info": {
   "name": "python"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
